{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll follow classification process with https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TripletMarginLossMNIST.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:VERSION 0.9.98\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.datasets import AG_NEWS, SogouNews\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import pytorch_metric_learning\n",
    "from pytorch_metric_learning import losses, miners, samplers, distances, testers, reducers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) # embedding + averaging\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return embedded\n",
    "    \n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cur_epoch, dataloader, mining_func):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(text, offsets)        \n",
    "        indices_tuple = mining_func(embeddings, label)\n",
    "        loss = loss_func(embeddings, label, indices_tuple)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "    print(\"Current Epoch {} : Loss = {}, Number of mined triplets = {}\".format(cur_epoch, loss, mining_func.num_triplets))\n",
    "\n",
    "def get_all_embeddings(loader, model):\n",
    "    model.eval()\n",
    "    emb_container = []\n",
    "    label_container = []\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(loader):\n",
    "        emb = model(text, offsets)\n",
    "        emb_container.append(emb.cpu())\n",
    "        label_container.append(label.cpu())\n",
    "\n",
    "    x,y = torch.cat(emb_container), torch.cat(label_container)\n",
    "    return x.detach(), y.detach()\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def evaluate(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(test_embeddings, \n",
    "                                                train_embeddings,\n",
    "                                                test_labels,\n",
    "                                                train_labels,\n",
    "                                                False)\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('.data/DBpedia/train.csv')\n",
    "test_df = pd.read_csv('.data/DBpedia/test.csv')\n",
    "valid_df = pd.read_csv('.data/DBpedia/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {v:k for k,v in enumerate(train_df['l3'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(list(zip(train_df['l3'].map(lambda x : label_dict[x]).tolist(), train_df['text'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
    "\n",
    "# set vocab size\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "628321\n"
     ]
    }
   ],
   "source": [
    "# load nn.Module object\n",
    "# train_iter = AG_NEWS(split='train')\n",
    "train_iter = iter(list(zip(train_df['l3'].tolist(), train_df['text'].tolist())))\n",
    "num_class = len(set([label for (label, text) in train_iter])); print(num_class)\n",
    "vocab_size = len(vocab); print(vocab_size)\n",
    "emsize = 32\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(list(zip(train_df['l3'].map(lambda x : label_dict[x]).tolist(), train_df['text'].tolist())))\n",
    "valid_iter = iter(list(zip(valid_df['l3'].map(lambda x : label_dict[x]).tolist(), valid_df['text'].tolist())))\n",
    "test_iter = iter(list(zip(test_df['l3'].map(lambda x : label_dict[x]).tolist(), test_df['text'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "valid_dataset = to_map_style_dataset(valid_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.ThresholdReducer(low = 0)\n",
    "loss_func = losses.TripletMarginLoss(margin = 0.2, distance = distance, reducer = reducer)\n",
    "mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semihard\")\n",
    "accuracy_calculator = AccuracyCalculator(include = (\"precision_at_1\",), k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch 1 : Loss = 0.08084768056869507, Number of mined triplets = 4\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.7220449386452611\n",
      "Current Epoch 2 : Loss = 0.08033990859985352, Number of mined triplets = 24\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.7787939599302562\n",
      "Current Epoch 3 : Loss = 0.09155242145061493, Number of mined triplets = 30\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.8117248412672303\n",
      "Current Epoch 4 : Loss = 0.0937754362821579, Number of mined triplets = 35\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.8277297101687666\n",
      "Current Epoch 5 : Loss = 0.0509636290371418, Number of mined triplets = 7\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.8434055992367668\n",
      "Current Epoch 6 : Loss = 0.0, Number of mined triplets = 0\n",
      "Computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running k-nn with k=20\n",
      "INFO:root:embedding dimensionality is 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Precision@1) = 0.8504786656577952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fdde7915a24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmining_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_calculator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-172d2e23fd44>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cur_epoch, dataloader, mining_func)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mindices_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmining_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/pytorch_metric_learning/miners/base_miner.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             )\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmining_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_assertion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmining_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/pytorch_metric_learning/miners/triplet_margin_miner.py\u001b[0m in \u001b[0;36mmine\u001b[0;34m(self, embeddings, labels, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         )\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0map_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0man_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/pytorch_metric_learning/distances/base_distance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_emb, ref_emb)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mquery_emb_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mref_emb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mref_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/pytorch_metric_learning/distances/base_distance.py\u001b[0m in \u001b[0;36mmaybe_normalize\u001b[0;34m(self, embeddings, dim, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmaybe_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/pytorch_metric_learning/distances/base_distance.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, embeddings, dim, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmaybe_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hfcp/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4428\u001b[0;31m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4430\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # epoch\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(epoch, train_dataloader, mining_func)\n",
    "    evaluate(train_dataloader, test_dataloader, model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate out of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train_data, out_test_data = AG_NEWS()\n",
    "out_train_dataset = to_map_style_dataset(out_train_data)\n",
    "out_test_dataset = to_map_style_dataset(out_test_data)\n",
    "out_train_dataloader = DataLoader(out_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "out_test_dataloader = DataLoader(out_test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {1 : 'World',\n",
    "2 : 'Sports',\n",
    "3 : 'Business',\n",
    "4 : \"Sci/Tec\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_embed=10000,\n",
    "                 d_embed=32,\n",
    "                 d_hidden1=64,\n",
    "                 d_hidden2=4,\n",
    "                 d_out=2,\n",
    "                 dp=0.2,\n",
    "                 pretrained=None):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        \n",
    "        self.embed = pretrained\n",
    "        self.dropout1 = nn.Dropout(dp)\n",
    "        self.bn1 = nn.BatchNorm1d(d_embed)\n",
    "        self.fc1 = nn.Linear(d_embed, d_hidden1)\n",
    "        self.dropout2 = nn.Dropout(dp)\n",
    "        self.bn2 = nn.BatchNorm1d(d_hidden1)\n",
    "        self.fc2 = nn.Linear(d_hidden1, d_hidden2)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        x = self.embed(text, offsets)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_model = FineTuningModel(pretrained=model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(fine_tuning_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(dataloader):\n",
    "    fine_tuning_model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        label = label - 1\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = fine_tuning_model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(fine_tuning_model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    fine_tuning_model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            label = label - 1\n",
    "            predited_label = fine_tuning_model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1875 batches | accuracy    0.712\n",
      "| epoch   1 |  1000/ 1875 batches | accuracy    0.806\n",
      "| epoch   1 |  1500/ 1875 batches | accuracy    0.834\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 18.77s | valid accuracy    0.871 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1875 batches | accuracy    0.863\n",
      "| epoch   2 |  1000/ 1875 batches | accuracy    0.867\n",
      "| epoch   2 |  1500/ 1875 batches | accuracy    0.873\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 17.61s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1875 batches | accuracy    0.882\n",
      "| epoch   3 |  1000/ 1875 batches | accuracy    0.886\n",
      "| epoch   3 |  1500/ 1875 batches | accuracy    0.885\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 16.27s | valid accuracy    0.891 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1875 batches | accuracy    0.890\n",
      "| epoch   4 |  1000/ 1875 batches | accuracy    0.890\n",
      "| epoch   4 |  1500/ 1875 batches | accuracy    0.894\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 16.36s | valid accuracy    0.901 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1875 batches | accuracy    0.899\n",
      "| epoch   5 |  1000/ 1875 batches | accuracy    0.899\n",
      "| epoch   5 |  1500/ 1875 batches | accuracy    0.899\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 18.60s | valid accuracy    0.899 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1875 batches | accuracy    0.907\n",
      "| epoch   6 |  1000/ 1875 batches | accuracy    0.906\n",
      "| epoch   6 |  1500/ 1875 batches | accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 17.93s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1875 batches | accuracy    0.908\n",
      "| epoch   7 |  1000/ 1875 batches | accuracy    0.910\n",
      "| epoch   7 |  1500/ 1875 batches | accuracy    0.908\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 18.76s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1875 batches | accuracy    0.911\n",
      "| epoch   8 |  1000/ 1875 batches | accuracy    0.911\n",
      "| epoch   8 |  1500/ 1875 batches | accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 16.13s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1875 batches | accuracy    0.910\n",
      "| epoch   9 |  1000/ 1875 batches | accuracy    0.913\n",
      "| epoch   9 |  1500/ 1875 batches | accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 18.00s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1875 batches | accuracy    0.910\n",
      "| epoch  10 |  1000/ 1875 batches | accuracy    0.911\n",
      "| epoch  10 |  1500/ 1875 batches | accuracy    0.911\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 18.57s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    fine_tune(out_train_dataloader)\n",
    "    accu_val = evaluate(out_test_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"valid accuracy\")\n",
    "plt.plot([0.871, 0.883 , 0.891 , 0.901, 0.899, 0.902, 0.906, 0.906, 0.905, 0.906], label='metric learning')\n",
    "plt.plot([0.835,0.854,0.879 ,0.889 , 0.889 ,0.893 ,0.894 ,0.893 ,0.900 ,0.900], label='classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
